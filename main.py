import os
import re
from typing import List, Dict, Any

import streamlit as st
from dotenv import load_dotenv

# --- LangChain types ---
try:
    from langchain_core.documents import Document
except Exception:  # older LC
    from langchain.schema import Document  # type: ignore

# --- LLM (OpenAI Chat) ---
_ChatOpenAI = None
try:
    from langchain_openai import ChatOpenAI as _ChatOpenAI  # >=0.1
except Exception:
    try:
        from langchain.chat_models import ChatOpenAI as _ChatOpenAI  # legacy
    except Exception:
        _ChatOpenAI = None

# --- Project modules (íŒŒì¼ëª… ë³€ê²½ ê¸ˆì§€) ---
from RAG.rag_pipeline import create_retriever  # í˜¸í™˜ ë˜í¼ê°€ rag_pipeline.pyì— ìˆì–´ì•¼ í•¨
from RAG.chain_builder import (
    extract_relevant_sentences,
    build_answer_from_sentences,
)
from file_handler import get_documents_from_files
from text_scraper import clean_html_parallel

# =========================
# App Config
# =========================
load_dotenv()
st.set_page_config(page_title="Perfecto RAG Chatbot", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ Perfecto RAG Chatbot")
st.caption("ì—…ë¡œë“œ/URLì„ ì¸ë±ì‹±í•˜ê³ , ì§ˆë¬¸ì— í•„ìš”í•œ **ìµœì†Œ ë¬¸ì¥**ë§Œ ê·¼ê±°ë¡œ ë‹µí•©ë‹ˆë‹¤. (YouTube=íƒ€ì„ì½”ë“œ, PDF=í˜ì´ì§€)")

# =========================
# Sidebar (UI ì „ìš©)
# =========================
with st.sidebar:
    st.subheader("âš™ï¸ ì„¤ì •")
    model_name = st.selectbox("LLM ëª¨ë¸", ["gpt-4o", "gpt-4o-mini", "gpt-4o-mini-2024-07-18"], index=1)
    topk_sent = st.slider("í‘œì‹œí•  ê·¼ê±° ë¬¸ì¥ ìˆ˜", 3, 12, 8, 1)
    generate_answer = st.checkbox("ìƒì„±í˜• ì‘ë‹µ ìƒì„± (LLM ì‚¬ìš©)", value=True)
    st.markdown("---")
    if os.environ.get("OPENAI_API_KEY"):
        st.success("OPENAI_API_KEY ê°ì§€ë¨")
    else:
        st.warning("OPENAI_API_KEY ë¯¸ì„¤ì • â€” LLM ì‘ë‹µì´ ë¹„í™œì„±í™”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# =========================
# Session State
# =========================
if "docs" not in st.session_state:
    st.session_state.docs: List[Document] = []
if "retriever" not in st.session_state:
    st.session_state.retriever = None

# =========================
# Inputs (UI ì „ìš©)
# =========================
col_left, col_right = st.columns([1, 1])

with col_left:
    st.markdown("#### ğŸ”— URL ì…ë ¥")
    url_text = st.text_area(
        "ì—¬ëŸ¬ ê°œë©´ ì¤„ë°”ê¿ˆ/ì‰¼í‘œë¡œ êµ¬ë¶„",
        height=120,
        placeholder="https://example.com/article\nhttps://youtu.be/xxxxxx\n...",
    )
    fetch_btn = st.button("ë¶ˆëŸ¬ì˜¤ê¸° / ì¸ë±ì‹±", use_container_width=True)

with col_right:
    st.markdown("#### ğŸ“„ íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "PDF, TXT, DOCX, MD, SRT/VTT ë“± (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
        type=["pdf", "txt", "md", "docx", "csv", "json", "srt", "vtt"],
        accept_multiple_files=True,
    )

st.markdown("---")
user_query = st.text_input("â“ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ) ì´ ë¬¸ì„œì—ì„œ ì œì•ˆí•œ í•µì‹¬ ìš”ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")

# =========================
# Helpers (UIì—ì„œ ì“°ëŠ” ìµœì†Œ ìœ í‹¸)
# =========================
def _normalize_urls(block: str) -> List[str]:
    if not block.strip():
        return []
    raw = [x.strip() for x in re.split(r"[\n,]+", block) if x.strip()]
    return list(dict.fromkeys([u for u in raw if re.match(r"^https?://", u)]))  # dedupe keep-order


def _docs_from_urls(urls: List[str]) -> List[Document]:
    if not urls:
        return []
    docs: List[Document] = []
    cleaned = clean_html_parallel(urls)  # ë ˆí¬ ì œê³µ í•¨ìˆ˜ (ë°˜í™˜ í¬ë§·ì— ê´€ëŒ€)
    if isinstance(cleaned, list) and all(isinstance(x, str) for x in cleaned):
        for u, txt in zip(urls, cleaned):
            docs.append(Document(page_content=txt or "", metadata={"source": u, "url": u}))
    elif isinstance(cleaned, list) and all(isinstance(x, dict) for x in cleaned):
        for item in cleaned:
            txt = item.get("text") or item.get("content") or ""
            meta: Dict[str, Any] = {k: v for k, v in item.items() if k not in ("text", "content")}
            if "source" not in meta and "url" not in meta:
                src = item.get("url") or item.get("canonical") or item.get("source")
                if src:
                    meta["source"] = src
            docs.append(Document(page_content=txt, metadata=meta))
    else:
        for u in urls:
            docs.append(Document(page_content=str(cleaned), metadata={"source": u, "url": u}))
    return docs


def _docs_from_files(files) -> List[Document]:
    if not files:
        return []
    return get_documents_from_files(files)

def _get_llm():
    if not generate_answer or _ChatOpenAI is None:
        return None
    try:
        return _ChatOpenAI(model=model_name, temperature=0.0)
    except Exception:
        return None

# =========================
# ì¸ë±ì‹± (UI íŠ¸ë¦¬ê±°)
# =========================
if fetch_btn:
    urls = _normalize_urls(url_text)
    url_docs = _docs_from_urls(urls)
    file_docs = _docs_from_files(uploaded_files)

    all_docs = file_docs + url_docs
    st.session_state.docs = all_docs

    if not all_docs:
        st.warning("ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. URL ë˜ëŠ” íŒŒì¼ì„ ì œê³µí•˜ì„¸ìš”.")
        st.session_state.retriever = None
    else:
        # retrieverëŠ” ì¦‰ì‹œ ë§Œë“¤ì–´ ì„¸ì…˜ì— ë³´ê´€ (rag_pipelineì˜ êµ¬í˜„ ì‹œê·¸ë‹ˆì²˜ì— ë§ì¶° source_documents í‚¤ë¥¼ ìš°ì„  ì‹œë„)
        try:
            st.session_state.retriever = create_retriever(source_documents=all_docs)
        except TypeError:
            st.session_state.retriever = create_retriever(all_docs)
        st.success(f"ì¸ë±ì‹± ì™„ë£Œ! ë¬¸ì„œ ìˆ˜: {len(all_docs)}")

# =========================
# í•µì‹¬ ë³€ê²½ ë¸”ë¡ ì ìš© (ì§ˆì˜ ì²˜ë¦¬)
# =========================
if user_query:
    # 1) ê²€ìƒ‰: ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ ìƒì„±(ë˜ëŠ” ì´ë¯¸ ë§Œë“  retriever ì‚¬ìš©)
    try:
        retriever = st.session_state.retriever or create_retriever()
    except Exception:
        retriever = st.session_state.retriever  # ì—†ìœ¼ë©´ None

    if retriever is None:
        if not st.session_state.docs:
            st.info("ë¨¼ì € 'ë¶ˆëŸ¬ì˜¤ê¸° / ì¸ë±ì‹±'ì„ ëˆŒëŸ¬ ë¬¸ì„œë¥¼ ë“±ë¡í•˜ì„¸ìš”.")
        docs_for_search = st.session_state.docs
    else:
        try:
            docs_for_search = retriever.get_relevant_documents(user_query)
        except Exception:
            # retrieverê°€ ë¬¸ì„œë¥¼ ë‚´ì¥í•˜ì§€ ì•ŠëŠ” êµ¬í˜„ì´ë¼ë©´ ì „ì²´ ë¬¸ì„œì—ì„œ í›„ì²˜ë¦¬
            docs_for_search = st.session_state.docs

    if not docs_for_search:
        st.warning("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # 2) ì§ˆë¬¸ì— í•„ìš”í•œ 'ìµœì†Œ' ë¬¸ì¥ë§Œ ì¶”ì¶œ
        picked = extract_relevant_sentences(user_query, docs_for_search, max_sentences=topk_sent)

        # 3) LLMì´ ì˜¤ì§ í•´ë‹¹ ë¬¸ì¥ë“¤ë§Œ ê·¼ê±°ë¡œ ë‹µë³€ ìƒì„±
        answer_text = ""
        llm = _get_llm()
        if generate_answer and llm is not None:
            try:
                answer_text = build_answer_from_sentences(llm, user_query, picked)
            except Exception:
                answer_text = ""

        # 4) UI ì¶œë ¥
        st.markdown("### ğŸ§  ë‹µë³€")
        if answer_text:
            st.write(answer_text)
        else:
            st.write("ìƒì„±í˜• ì‘ë‹µ ë¹„í™œì„±í™”ë¨. ì•„ë˜ **ê·¼ê±° ë¬¸ì¥**ì„ ì°¸ê³ í•˜ì„¸ìš”.")

        with st.expander("ğŸ” ì¶œì²˜(ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ìµœì†Œ ë¬¸ì¥)"):
            for i, it in enumerate(picked, 1):
                st.markdown(
                    f"**S{i}.** {it['text']}\n\n"
                    f"<small>ì¶œì²˜: {it['citation']}</small>",
                    unsafe_allow_html=True,
                )
