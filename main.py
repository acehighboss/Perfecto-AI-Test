from __future__ import annotations

import os
import re
import requests
from typing import List
import streamlit as st

from langchain_core.documents import Document

# ë ˆí¬ ë‚´ë¶€ ëª¨ë“ˆ
from RAG.rag_pipeline import get_retriever_from_source
from RAG.chain_builder import (
    extract_relevant_sentences,
    build_answer_from_sentences,
    get_conversational_rag_chain,  # ë ˆí¬ í˜¸í™˜ (ë¯¸ì‚¬ìš© ì‹œ ì‚­ì œ ê°€ëŠ¥)
    get_default_chain,             # ë ˆí¬ í˜¸í™˜ (ë¯¸ì‚¬ìš© ì‹œ ì‚­ì œ ê°€ëŠ¥)
)

# LLM (í™˜ê²½ì— ë§ê²Œ êµì²´ ê°€ëŠ¥)
try:
    from langchain_openai import ChatOpenAI
    LLM_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    llm = ChatOpenAI(model=LLM_MODEL, temperature=0)
except Exception:
    llm = None  # í´ë°±: build_answer_from_sentencesì—ì„œ ì•ˆì „ ì²˜ë¦¬


st.set_page_config(page_title="RAG Chatbot", page_icon="ğŸ§ ", layout="wide")
st.title("ğŸ§  ì¶œì²˜ ê¸°ë°˜ RAG ì±—ë´‡ (ìƒì„± ìš”ì•½ ê¸°ë³¸ ON)")

# ======================
# Sidebar Controls
# ======================
st.sidebar.header("ì‘ë‹µ ì„¤ì •")
ALLOW_GENERATION = st.sidebar.checkbox("ìƒì„± ìš”ì•½ ì¼œê¸°", value=True)
SHOW_MINIMAL_EVIDENCE = st.sidebar.checkbox("ì¶œì²˜ëŠ” ìµœì†Œ ë¬¸ì¥ë§Œ í‘œì‹œ", value=True)
TOP_K_SENT = st.sidebar.slider("ì¶œì²˜ ë¬¸ì¥ ê°œìˆ˜(k)", min_value=3, max_value=15, value=8, step=1)

st.sidebar.markdown("---")
st.sidebar.subheader("ë¬¸ì„œ ì—…ë¡œë“œ")
uploaded_files = st.sidebar.file_uploader(
    "í…ìŠ¤íŠ¸/PDF/ìë§‰ ë“± (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)", accept_multiple_files=True
)

st.sidebar.subheader("URL ì¶”ê°€")
urls_text = st.sidebar.text_area("ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—¬ëŸ¬ URL ì…ë ¥ ê°€ëŠ¥", height=120, placeholder="https://example.com/article-1\nhttps://example.com/article-2")

# ======================
# Load Documents
# ======================

def _load_from_uploaded(files) -> List[Document]:
    docs: List[Document] = []
    for f in files or []:
        name = getattr(f, "name", "uploaded")
        try:
            content_bytes = f.read()
            try:
                text = content_bytes.decode("utf-8", errors="ignore")
            except Exception:
                text = content_bytes.decode("cp949", errors="ignore")
            text = re.sub(r"\s+", " ", text).strip()
            if not text:
                continue
            docs.append(Document(page_content=text, metadata={"source": f"file://{name}"}))
        except Exception as e:
            st.sidebar.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {name} ({e})")
    return docs


def _fetch_url_text(url: str) -> str:
    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        # ì•„ì£¼ ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ(ë ˆí¬ ë‚´ text_scraperê°€ ìˆë‹¤ë©´ êµì²´ ì‚¬ìš© ê¶Œì¥)
        html = resp.text
        # ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ì œê±°
        html = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", html)
        # íƒœê·¸ ì œê±°
        text = re.sub(r"(?is)<[^>]+>", " ", html)
        text = re.sub(r"\s+", " ", text)
        return text.strip()
    except Exception as e:
        st.sidebar.warning(f"URL ìš”ì²­ ì‹¤íŒ¨: {url} ({e})")
        return ""


def _load_from_urls(urls_raw: str) -> List[Document]:
    docs: List[Document] = []
    urls = [u.strip() for u in (urls_raw or "").splitlines() if u.strip()]
    for u in urls:
        txt = _fetch_url_text(u)
        if not txt:
            continue
        docs.append(Document(page_content=txt, metadata={"source": u, "url": u}))
    return docs


# ë¬¸ì„œ í•©ì¹˜ê¸° & ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì„±
docs_all: List[Document] = []
docs_all.extend(_load_from_uploaded(uploaded_files))
docs_all.extend(_load_from_urls(urls_text))

if "retriever" not in st.session_state or st.button("ğŸ”„ ë¦¬íŠ¸ë¦¬ë²„ ì¬êµ¬ì„±"):
    st.session_state["retriever"] = get_retriever_from_source(docs_all, top_k=6)
retriever = st.session_state["retriever"]

# ======================
# Chat Section
# ======================

question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ) SKí…”ë ˆì½¤ì´ ìì²´ ê°œë°œí•œ LLMì˜ ì´ë¦„ê³¼ íŠ¹ì§•ì€?")

if question:
    # 1) Retrieve
    docs = retriever.get_relevant_documents(question)

    # 2) Extract minimal evidence sentences
    sentences = extract_relevant_sentences(docs, question, k=TOP_K_SENT, dedupe=True)

    # 3) Build answer from sentences
    result = build_answer_from_sentences(
        llm, question, sentences,
        allow_generation=ALLOW_GENERATION
    )

    st.subheader("ğŸ§  ë‹µë³€")
    st.markdown(result["answer"])

    st.subheader("ğŸ” ì¶œì²˜(ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ìµœì†Œ ë¬¸ì¥)")
    if SHOW_MINIMAL_EVIDENCE:
        for i, s in enumerate(result["sentences"], 1):
            extras = []
            if s.get("page"):
                extras.append(f"p.{s['page']}")
            if s.get("timecode"):
                extras.append(str(s["timecode"]))
            suffix = f" ({', '.join(extras)})" if extras else ""
            st.markdown(f"**S{i}.** {s['text']}\n\nì¶œì²˜: {s.get('source','')}{suffix}")
    else:
        st.info("ì¶œì²˜ í‘œì‹œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì¼¤ ìˆ˜ ìˆì–´ìš”.")
