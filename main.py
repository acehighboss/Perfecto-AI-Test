import os
import re
import traceback
import streamlit as st
from typing import List, Any, Dict
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.documents import Document

# ---- ë‚´ë¶€ ëª¨ë“ˆ ----
from file_handler import (
    get_documents_from_urls_robust,
    get_documents_from_uploaded_files,
)
from RAG.rag_pipeline import get_retriever_from_source
from RAG.chain_builder import get_conversational_rag_chain


# -----------------------------
# ìœ í‹¸: ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„í•  & ì„œí¬íŠ¸ ë¬¸ì¥ ì¶”ì¶œ
# -----------------------------
_SENT_SPLIT = re.compile(r"(?<=[.!?ã€‚!?])\s+|[\n]+")

def _split_sentences(text: str) -> List[str]:
    sents = [s.strip() for s in _SENT_SPLIT.split(text or "") if s.strip()]
    return sents[:2000]  # ê³¼ë„í•œ ê¸¸ì´ ë°©ì§€

def _score_sentence(sent: str, query: str, answer: str) -> float:
    # ì•„ì£¼ ê°€ë²¼ìš´ í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜(ì§ˆë¬¸+ë‹µë³€ í‚¤ì›Œë“œì™€ì˜ êµì§‘í•© í¬ê¸°)
    def toks(x: str) -> set:
        return set(re.findall(r"[A-Za-z0-9ê°€-í£]+", (x or "").lower()))
    q = toks(query)
    a = toks(answer)
    s = toks(sent)
    inter = len((q | a) & s)
    # ê¸¸ì´ ë³´ì •(ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ê°ì )
    penalty = max(1.0, len(sent) / 300.0)
    return inter / penalty

def extract_support_sentences(doc_text: str, query: str, answer: str, topk: int = 3) -> List[str]:
    sents = _split_sentences(doc_text)
    scored = sorted(sents, key=lambda s: _score_sentence(s, query, answer), reverse=True)
    uniq, res = set(), []
    for s in scored:
        key = s[:120]
        if key not in uniq and len(s) > 10:
            uniq.add(key)
            res.append(s)
        if len(res) >= topk:
            break
    return res

# ------------------------------------
# Streamlit ê¸°ë³¸ ì„¤ì •
# ------------------------------------
st.set_page_config(page_title="Perfecto AI Test (RAG)", page_icon="ğŸ§ª", layout="wide")
st.title("Perfecto AI Test (RAG)")

# ------------------------------------
# Session State ì´ˆê¸°í™”
# ------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages: List[Any] = []
if "docs" not in st.session_state:
    st.session_state.docs: List[Document] = []
if "ready_to_analyze" not in st.session_state:
    st.session_state.ready_to_analyze = False
if "docs_for_citation" not in st.session_state:
    st.session_state.docs_for_citation: List[Document] = []
if "last_answer" not in st.session_state:
    st.session_state.last_answer = ""

# ------------------------------------
# ë ˆì´ì•„ì›ƒ: ì¢Œì¸¡ ì‚¬ì´ë“œë°”(ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸/ì†ŒìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°)
# ------------------------------------
with st.sidebar:
    st.subheader("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸(í˜ë¥´ì†Œë‚˜)")
    system_prompt = st.text_area(
        "ëª¨ë¸ì˜ ì—­í• /í†¤/ìŠ¤íƒ€ì¼ì„ ì •ì˜í•˜ì„¸ìš”",
        value=(
            "ë„ˆëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë¶„ì„ê°€ë‹¤. ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ê³ , "
            "ì‚¬ì‹¤ ê·¼ê±°ë¥¼ ë°íˆë©°, ë¶ˆí™•ì‹¤í•˜ë©´ ì¶”ì •í•˜ì§€ ì•ŠëŠ”ë‹¤."
        ),
        height=140,
    )

    st.markdown("---")
    st.subheader("ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°")

    url_input = st.text_area(
        "URL (ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—¬ëŸ¬ ê°œ ì…ë ¥)",
        value="https://www.mobiinside.co.kr/2025/06/27/ai-news-3/\nhttps://namu.wiki/w/%EC%84%B1%EA%B2%BD",
        height=100,
    )

    uploaded_files = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ (PDF/DOCX/TXT/MD/CSV/JSON/LOG)",
        type=["pdf", "docx", "txt", "md", "csv", "json", "log"],
        accept_multiple_files=True,
    )

    respect_robots = st.toggle("robots.txt ì¤€ìˆ˜", value=True)
    use_js_render = st.toggle("JS ë Œë”ë§(Playwright) ì‚¬ìš©", value=False,
                              help="CSR ì‚¬ì´íŠ¸ ëŒ€ì‘(ëŠë¦¼). í™˜ê²½ì— ë”°ë¼ ë¯¸ë™ì‘ ê°€ëŠ¥")
    js_only_when_needed = st.toggle("ì •ì  ì¶”ì¶œ ì‹¤íŒ¨ ì‹œì—ë§Œ JS ì‚¬ìš©", value=True)

    if st.button("ë¶ˆëŸ¬ì˜¤ê¸°", use_container_width=True):
        try:
            urls = [u.strip() for u in url_input.splitlines() if u.strip()]
            url_docs = get_documents_from_urls_robust(
                urls,
                respect_robots=respect_robots,
                use_js_render=use_js_render,
                js_only_when_needed=js_only_when_needed,
            ) if urls else []

            file_docs = get_documents_from_uploaded_files(uploaded_files) if uploaded_files else []

            docs = (url_docs or []) + (file_docs or [])
            st.session_state.docs = docs
            st.session_state.docs_for_citation = docs
            st.session_state.ready_to_analyze = len(docs) > 0

            if not docs:
                st.warning("ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. URL ë˜ëŠ” íŒŒì¼ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
            else:
                st.success(f"{len(docs)}ê°œ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
            st.caption(traceback.format_exc())
            st.session_state.docs = []
            st.session_state.ready_to_analyze = False

# ------------------------------------
# ë©”ì¸ ì˜ì—­: ìƒë‹¨ Q/A, í•˜ë‹¨ ì…ë ¥ì°½
# ------------------------------------
# ìƒë‹¨(ì™¼ìª½: ëŒ€í™”/Q&A, ì˜¤ë¥¸ìª½: ì†ŒìŠ¤ ë¯¸ë¦¬ë³´ê¸°)
col_left, col_right = st.columns([3, 2])

with col_left:
    st.subheader("ì§ˆë¬¸ & ë‹µë³€")

    # ê¸°ì¡´ ê¸°ë¡ ë Œë”(ì§ˆë¬¸ì€ ì˜¤ë¥¸ìª½ ì˜ì—­ì—, ì‚¬ì´ë“œë°”ê°€ ì•„ë‹Œ ë³¸ë¬¸ì— í‘œì‹œ)
    for m in st.session_state.messages:
        if isinstance(m, HumanMessage):
            st.chat_message("user").write(m.content)
        elif isinstance(m, AIMessage):
            st.chat_message("assistant").write(m.content)

    # ìµœì‹  ë¶„ì„ ê²°ê³¼ì˜ ì¶œì²˜ JSON(ìš”ì²­ëœ í˜•íƒœ) ë Œë”
    if st.session_state.last_answer and st.session_state.docs_for_citation:
        question_for_json = None
        # ìµœê·¼ ì‚¬ìš©ì ë©”ì‹œì§€(ì§ˆë¬¸) ì¶”ì¶œ
        for msg in reversed(st.session_state.messages):
            if isinstance(msg, HumanMessage):
                question_for_json = msg.content
                break
        citation_json: Dict[str, Any] = {
            "question": question_for_json or "",
            "answer": st.session_state.last_answer,
            "sources": []
        }
        query = question_for_json or ""
        answer = st.session_state.last_answer

        # source_documentsê°€ ì²´ì¸ì—ì„œ ì£¼ì–´ì¡Œë‹¤ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©í•˜ê³ ,
        # ì—†ìœ¼ë©´ docs_for_citation(=í˜„ì¬ ì½”í¼ìŠ¤)ì—ì„œ ìƒìœ„ nê°œë§Œ ìƒ˜í”Œë¡œ í‘œì‹œ
        preview_docs = st.session_state.docs_for_citation
        max_sources = min(6, len(preview_docs))
        for d in preview_docs[:max_sources]:
            meta = d.metadata or {}
            title = meta.get("title") or meta.get("filename") or "unknown"
            src = meta.get("source") or ""
            support = extract_support_sentences(d.page_content or "", query, answer, topk=3)
            citation_json["sources"].append({
                "title": title,
                "source": src,
                "support": support,
            })

        st.markdown("**ì¶œì²˜ (JSON)**")
        st.json(citation_json)

with col_right:
    st.subheader("ì†ŒìŠ¤ ë¯¸ë¦¬ë³´ê¸°")
    preview_docs = st.session_state.docs_for_citation or st.session_state.docs
    if preview_docs:
        for i, d in enumerate(preview_docs[:8], 1):
            src = (d.metadata or {}).get("source", "")
            title = (d.metadata or {}).get("title", src or f"ë¬¸ì„œ {i}")
            with st.expander(f"[{i}] {title}"):
                if src:
                    st.caption(src)
                body = d.page_content or ""
                st.write(body[:1200] + ("..." if len(body) > 1200 else ""))
    else:
        st.caption("ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

# -----------------------------
# ìš°ì¸¡ í•˜ë‹¨ ì…ë ¥ì°½(ì‚¬ì´ë“œë°” X)
# -----------------------------
user_query = st.chat_input("ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")  # í™”ë©´ ìš°ì¸¡ í•˜ë‹¨ì— ê³ ì •

if user_query and st.session_state.ready_to_analyze and st.session_state.docs:
    # ì§ˆë¬¸ ë©”ì‹œì§€ ë³¸ë¬¸ ì˜ì—­ì— í‘œì‹œ
    st.session_state.messages.append(HumanMessage(content=user_query))

    try:
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì§ˆë¬¸ ì•ì— ì£¼ì…(ì²´ì¸ì´ system ì—­í• ì„ ì§ì ‘ ë°›ì§€ ì•ŠëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„)
        composed_query = f"[SYSTEM]\n{system_prompt}\n\n[USER]\n{user_query}"

        retriever = get_retriever_from_source(documents=st.session_state.docs)
        chain = get_conversational_rag_chain(retriever=retriever)

        with st.spinner("ë¶„ì„ ì¤‘..."):
            result = chain.invoke({"question": composed_query})

        answer_text = None
        source_docs: List[Document] = []

        if isinstance(result, dict):
            answer_text = result.get("answer") or result.get("output") or result.get("text")
            sd = result.get("source_documents") or result.get("sources") or []
            if isinstance(sd, list):
                for d in sd:
                    if isinstance(d, Document):
                        source_docs.append(d)
                    elif isinstance(d, dict) and "page_content" in d:
                        md = d.get("metadata", {}) or {}
                        source_docs.append(Document(page_content=d["page_content"], metadata=md))
        elif isinstance(result, str):
            answer_text = result

        if not answer_text:
            answer_text = "ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸/ë¬¸ì„œ ìƒíƒœë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”."

        # ë‹µë³€ì„ ë³¸ë¬¸ ì˜ì—­ì— í‘œì‹œ
        st.chat_message("assistant").write(answer_text)
        st.session_state.messages.append(AIMessage(content=answer_text))
        st.session_state.last_answer = answer_text

        # ì¶œì²˜ í‘œì‹œìš© ìƒíƒœ ì—…ë°ì´íŠ¸
        if source_docs:
            st.session_state.docs_for_citation = source_docs
        else:
            # ì²´ì¸ì´ ì¶œì²˜ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì€ ê²½ìš°ì—” í˜„ì¬ ì½”í¼ìŠ¤ë¡œ ëŒ€ì²´
            st.session_state.docs_for_citation = st.session_state.docs

    except Exception as e:
        st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        st.caption(traceback.format_exc())

elif user_query and not st.session_state.ready_to_analyze:
    st.warning("ë¨¼ì € ì¢Œì¸¡ì—ì„œ URL/íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ì„¸ìš”.")

