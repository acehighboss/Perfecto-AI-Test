from __future__ import annotations

import os
import re
import time
from typing import List, Dict, Any, Optional

import streamlit as st
from dotenv import load_dotenv

# LangChain types
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# RAG pipeline (search_type="similarity" ìœ ì§€)
from RAG.rag_pipeline import build_faiss_vectorstore, get_retriever_from_source
from RAG.chain_builder import get_conversational_rag_chain, get_default_chain

# íŒŒì¼ ì—…ë¡œë“œ â†’ Document ë³€í™˜ (repoì— ì¡´ì¬í•œë‹¤ê³  ê°€ì •)
try:
    from file_handler import get_documents_from_files
except Exception:
    get_documents_from_files = None

# URL í¬ë¡¤ë§ ì‹œë„ (repoì˜ ìŠ¤í¬ë ˆì´í¼ ìš°ì„  ì‚¬ìš©)
def load_urls_as_documents(urls: List[str]) -> List[Document]:
    urls = [u.strip() for u in urls if u and u.strip()]
    if not urls:
        return []

    # 1) ìš°ì„  repoì˜ text_scraper ì‚¬ìš©
    try:
        from text_scraper import clean_html_parallel, filter_noise
        rows = clean_html_parallel(urls)  # repo êµ¬í˜„ì²´ ê°€ì •: [{url, title, text, ...}, ...]
        docs: List[Document] = []
        for row in rows:
            txt = row.get("text") or ""
            txt = filter_noise(txt) if "filter_noise" in globals() else txt
            title = row.get("title") or ""
            source = row.get("url") or row.get("source") or ""
            if not txt.strip():
                continue
            meta = {
                "source": source,
                "title": title,
                "type": "web",
            }
            docs.append(Document(page_content=txt, metadata=meta))
        return docs
    except Exception:
        pass

    # 2) í´ë°±: requests + BeautifulSoup
    docs: List[Document] = []
    try:
        import requests
        from bs4 import BeautifulSoup
        for u in urls:
            try:
                r = requests.get(u, timeout=20, headers={"User-Agent": "Mozilla/5.0"})
                r.raise_for_status()
                soup = BeautifulSoup(r.text, "html.parser")
                # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°„ë‹¨ ë²„ì „)
                for s in soup(["script", "style", "noscript"]):
                    s.extract()
                title = soup.title.string.strip() if soup.title and soup.title.string else u
                text = re.sub(r"\s+", " ", soup.get_text(" ", strip=True)).strip()
                if not text:
                    continue
                docs.append(Document(page_content=text, metadata={"source": u, "title": title, "type": "web"}))
            except Exception:
                continue
    except Exception:
        pass

    return docs


# ============ Streamlit App ============

load_dotenv()
st.set_page_config(page_title="RAG Chatbot", page_icon="ğŸ¤–", layout="wide")

# --- Session State ---
if "ALL_DOCS" not in st.session_state:
    st.session_state.ALL_DOCS: List[Document] = []
if "VECTORSTORE" not in st.session_state:
    st.session_state.VECTORSTORE = None
if "RETRIEVER" not in st.session_state:
    st.session_state.RETRIEVER = None
if "CHAT_HISTORY" not in st.session_state:
    st.session_state.CHAT_HISTORY = []  # [({"role": "user"/"assistant"}, "content"), ...]
if "SOURCES_CACHE" not in st.session_state:
    st.session_state.SOURCES_CACHE: List[Dict[str, Any]] = []
if "INDEX_BUILT_AT" not in st.session_state:
    st.session_state.INDEX_BUILT_AT = None

# --- Sidebar Controls ---
st.sidebar.title("âš™ï¸ Settings")

# ì¸ë±ì‹± íŒŒë¼ë¯¸í„°
chunk_size = st.sidebar.number_input("Chunk size", min_value=200, max_value=2000, value=1000, step=50)
chunk_overlap = st.sidebar.number_input("Chunk overlap", min_value=0, max_value=400, value=120, step=10)

# RAG Debug
debug_rag = st.sidebar.checkbox("RAG Debug", value=False)

# ì¸ë±ìŠ¤ ê´€ë¦¬
col_reset1, col_reset2 = st.sidebar.columns(2)
with col_reset1:
    if st.button("ğŸ—‘ï¸ ì¸ë±ìŠ¤ ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.ALL_DOCS = []
        st.session_state.VECTORSTORE = None
        st.session_state.RETRIEVER = None
        st.session_state.SOURCES_CACHE = []
        st.session_state.INDEX_BUILT_AT = None
        st.success("ì¸ë±ìŠ¤ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")
with col_reset2:
    if st.button("ğŸ§¹ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
        st.session_state.CHAT_HISTORY = []
        st.success("ëŒ€í™” ê¸°ë¡ì„ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

st.sidebar.markdown("---")
st.sidebar.caption("í˜„ì¬ ë¬¸ì„œ ìˆ˜: **{}**".format(len(st.session_state.ALL_DOCS)))
if st.session_state.INDEX_BUILT_AT:
