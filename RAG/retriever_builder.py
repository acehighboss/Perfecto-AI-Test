import asyncio
import spacy
from langchain_core.documents import Document as LangChainDocument
from langchain_core.runnables import RunnableLambda
# â–¼â–¼â–¼ [ìˆ˜ì •] OpenAIEmbeddings ì„í¬íŠ¸ â–¼â–¼â–¼
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_cohere import CohereRerank
from langchain.retrievers import EnsembleRetriever

from .rag_config import RAGConfig
from .redis_cache import get_from_cache, set_to_cache, create_cache_key

# spaCy ì–¸ì–´ ëª¨ë¸ ë¡œë“œ (ì•± ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ)
try:
    nlp_korean = spacy.load("ko_core_news_sm")
    nlp_english = spacy.load("en_core_web_sm")
    print("âœ… spaCy language models loaded successfully.")
except OSError:
    print("âš ï¸ spaCy ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'requirements.txt'ì— ëª¨ë¸ì´ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    nlp_korean, nlp_english = None, None

def _split_documents_into_sentences(documents: list[LangChainDocument]) -> list[LangChainDocument]:
    """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ spaCyë¥¼ ì´ìš©í•´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    sentences = []
    if not nlp_korean or not nlp_english:
        print("spaCy ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ë¬¸ì¥ ë¶„í• ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return documents

    for doc in documents:
        if not doc.page_content or not doc.page_content.strip():
            continue

        # ë¨¼ì € í•œêµ­ì–´ ëª¨ë¸ë¡œ ì‹œë„
        nlp_doc = nlp_korean(doc.page_content)
        # íœ´ë¦¬ìŠ¤í‹±: í•œêµ­ì–´ ë¬¸ì¥ì´ ê±°ì˜ ì—†ìœ¼ë©´(ì•ŒíŒŒë²³ ë¹„ìœ¨ì´ ë†’ìœ¼ë©´) ì˜ì–´ ëª¨ë¸ ì¬ì‹œë„
        try:
            sents_list = list(nlp_doc.sents)
            if len(sents_list) <= 1 and sum(c.isalpha() and 'a' <= c.lower() <= 'z' for c in doc.page_content) / len(doc.page_content) > 0.5:
                nlp_doc = nlp_english(doc.page_content)
        except ZeroDivisionError: # ë¹ˆ page_contentì— ëŒ€í•œ ì˜ˆì™¸ ì²˜ë¦¬
            continue

        for sent in nlp_doc.sents:
            if sent.text.strip():
                sentences.append(LangChainDocument(page_content=sent.text.strip(), metadata=doc.metadata.copy()))

    # â–¼â–¼â–¼ [ë””ë²„ê¹… ì½”ë“œ] â–¼â–¼â–¼
    print("\n\n" + "="*50, flush=True)
    print("ğŸ•µï¸ 2. [retriever_builder] ë¬¸ì¥ ë¶„í•  ê²°ê³¼ í™•ì¸", flush=True)
    print(f"ì´ {len(sentences)}ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.", flush=True)
    eps_sentences = [s.page_content for s in sentences if "EPS" in s.page_content]
    if eps_sentences:
        print("âœ… EPS ê´€ë ¨ ë¬¸ì¥ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤:", flush=True)
        for sent in eps_sentences:
            print(f"   - {sent}", flush=True)
    else:
        print("ğŸš¨ ê²½ê³ : ë¬¸ì¥ ë¶„í•  í›„ EPS ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", flush=True)
    print("="*50 + "\n\n", flush=True)
    # â–²â–²â–² [ë””ë²„ê¹… ì½”ë“œ] â–²â–²â–²
    
    return sentences


def build_retriever(documents: list[LangChainDocument]):
    """
    ë¬¸ì„œë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•´í•˜ê³ , í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(BM25 + FAISS) ë° Rerankë¥¼ ìˆ˜í–‰í•˜ëŠ”
    ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    """
    if not documents:
        return None

    # 1. ë¬¸ì„œ ì „ì²´ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• 
    print("\n[1ë‹¨ê³„: ë¬¸ì„œ ì „ì²´ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• ]")
    sentences = _split_documents_into_sentences(documents)
    if not sentences:
        print("ë¶„í• ëœ ë¬¸ì¥ì´ ì—†ì–´ Retrieverë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    print(f"ì´ {len(sentences)}ê°œì˜ ë¬¸ì¥ ìƒì„± ì™„ë£Œ.")

    # â–¼â–¼â–¼ [ìˆ˜ì •] Google ì„ë² ë”©ì„ OpenAI ì„ë² ë”©ìœ¼ë¡œ êµì²´ â–¼â–¼â–¼
    # 2. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ(FAISS) ìƒì„±
    print("\n[2ë‹¨ê³„: ë¬¸ì¥ ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± (OpenAI)]")
    # ëª¨ë¸ ì´ë¦„ì€ í•„ìš”ì— ë”°ë¼ ë³€ê²½ ê°€ëŠ¥ (ì˜ˆ: "text-embedding-3-small")
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") 
    try:
        vectorstore = FAISS.from_documents(sentences, embeddings)
        faiss_retriever = vectorstore.as_retriever(search_kwargs={"k": RAGConfig.BM25_TOP_K})
    except Exception as e:
        print(f"FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return None
    
    # 3. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰(BM25) Retriever ìƒì„±
    print("\n[3ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ BM25 Retriever ìƒì„±]")
    bm25_retriever = BM25Retriever.from_documents(sentences)
    bm25_retriever.k = RAGConfig.BM25_TOP_K

    # 4. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìœ„í•œ EnsembleRetriever ìƒì„±
    print("\n[4ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ Ensemble Retriever êµ¬ì„±]")
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=[0.5, 0.5]
    )

    # 5. Cohere Rerank ì••ì¶•ê¸° ì„¤ì •
    print("\n[5ë‹¨ê³„: Cohere Reranker êµ¬ì„±]")
    cohere_reranker = CohereRerank(model="rerank-multilingual-v3.0", top_n=RAGConfig.RERANK_1_TOP_N)

    # 6. ìµœì¢… íŒŒì´í”„ë¼ì¸ ì²´ì¸ êµ¬ì„±
    def get_cached_or_run_pipeline(query: str):
        cache_key = create_cache_key("final_rag_result_openai", query) # ìºì‹œ í‚¤ ë³€ê²½
        
        cached_docs = get_from_cache(cache_key)
        if cached_docs is not None:
            return cached_docs

        print(f"\n[Cache Miss] ì§ˆë¬¸ '{query}'ì— ëŒ€í•œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (OpenAI)")
        
        retrieved_docs = ensemble_retriever.invoke(query)
        print(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í›„ {len(retrieved_docs)}ê°œ ë¬¸ì¥ ì„ ë³„ ì™„ë£Œ.")

        reranked_docs = cohere_reranker.compress_documents(documents=retrieved_docs, query=query)
        print(f"Cohere Rerank í›„ {len(reranked_docs)}ê°œ ë¬¸ì¥ ì„ ë³„ ì™„ë£Œ.")
        
        final_docs = [
            doc for doc in reranked_docs 
            if doc.metadata.get('relevance_score', 0) >= RAGConfig.RERANK_2_THRESHOLD
        ][:RAGConfig.FINAL_DOCS_COUNT]

        print(f"ìµœì¢… {len(final_docs)}ê°œ ë¬¸ì¥ ì„ ë³„ ì™„ë£Œ.")

        # â–¼â–¼â–¼ [ë””ë²„ê¹… ì½”ë“œ] â–¼â–¼â–¼
        print(f"\n\n{'='*50}")
        print(f"ğŸ•µï¸ 3. [retriever_builder] RAG íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ê²°ê³¼ í™•ì¸")
        print(f"ì§ˆë¬¸: {query}")
        print(f"{'-'*50}")

        retrieved_docs = ensemble_retriever.invoke(query)
        print(f"â¡ï¸ [ë‹¨ê³„ 1] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(BM25+FAISS) ê²°ê³¼: {len(retrieved_docs)}ê°œ ë¬¸ì„œ")
        eps_in_retrieved = [doc for doc in retrieved_docs if "EPS" in doc.page_content]
        if eps_in_retrieved:
            print(f"âœ… ì´ ì¤‘ {len(eps_in_retrieved)}ê°œ ë¬¸ì„œì— EPS ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            print("ğŸš¨ ê²½ê³ : í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ EPS ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print(f"{'-'*50}")

        reranked_docs = cohere_reranker.compress_documents(documents=retrieved_docs, query=query)
        print(f"â¡ï¸ [ë‹¨ê³„ 2] Cohere Rerank ê²°ê³¼: {len(reranked_docs)}ê°œ ë¬¸ì„œ")
        eps_in_reranked = [doc for doc in reranked_docs if "EPS" in doc.page_content]
        if eps_in_reranked:
            print(f"âœ… Rerank í›„ì—ë„ {len(eps_in_reranked)}ê°œ ë¬¸ì„œì— EPS ì •ë³´ê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
            for doc in eps_in_reranked:
                 print(f"   - (ì ìˆ˜: {doc.metadata.get('relevance_score', 'N/A')}) {doc.page_content}")
        else:
            print("ğŸš¨ ê²½ê³ : Rerank ê³¼ì •ì—ì„œ EPS ê´€ë ¨ ë¬¸ì„œê°€ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"{'-'*50}")
        
        final_docs = [
            doc for doc in reranked_docs 
            if doc.metadata.get('relevance_score', 0) >= RAGConfig.RERANK_2_THRESHOLD
        ][:RAGConfig.FINAL_DOCS_COUNT]

        print(f"â¡ï¸ [ë‹¨ê³„ 3] ìµœì¢… í•„í„°ë§(ì ìˆ˜ >= {RAGConfig.RERANK_2_THRESHOLD}) ê²°ê³¼: {len(final_docs)}ê°œ ë¬¸ì„œ")
        eps_in_final = [doc for doc in final_docs if "EPS" in doc.page_content]
        if eps_in_final:
            print(f"âœ… ìµœì¢… ë¬¸ì„œì— EPS ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        else:
            print("ğŸš¨ ë¬¸ì œ ì§€ì : ìµœì¢… í•„í„°ë§ ê³¼ì •ì—ì„œ EPS ê´€ë ¨ ë¬¸ì„œê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. RERANK_2_THRESHOLD ê°’ì„ ë‚®ì¶°ë³´ëŠ” ê²ƒì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        print(f"{'='*50}\n\n")
        # â–²â–²â–² [ë””ë²„ê¹… ì½”ë“œ] â–²â–²â–²
        
        set_to_cache(cache_key, final_docs)
        
        return final_docs

    return RunnableLambda(get_cached_or_run_pipeline)
