import asyncio
import bs4
from playwright.async_api import async_playwright
from langchain_core.documents import Document as LangChainDocument

from file_handler import get_documents_from_files

async def _scrape_url_with_playwright(url: str) -> list[LangChainDocument]:
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ URLì˜ ë™ì  ì½˜í…ì¸ ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.
    """
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch()
            page = await browser.new_page()
            
            # ë„¤íŠ¸ì›Œí¬ê°€ ì•ˆì •ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ ë™ì  ì½˜í…ì¸  ë¡œë”© ë³´ì¥
            await page.goto(url, wait_until="networkidle") 
            
            # í˜ì´ì§€ ì œëª©ê³¼ ë Œë”ë§ëœ HTML ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
            title = await page.title()
            html_content = await page.content()
            
            await browser.close()

            # BeautifulSoupìœ¼ë¡œ HTML ì •ì œ
            soup = bs4.BeautifulSoup(html_content, "lxml")
            
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
            for element in soup.select("script, style, nav, footer, aside, .ad, .advertisement, .banner, .menu, .header, .footer"):
                element.decompose()

            # ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ì„ ìš°ì„ ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content_container = soup.find("main") or soup.find("article") or soup.find("div", class_="content") or soup.find("body")
            cleaned_text = content_container.get_text(separator="\n", strip=True) if content_container else ""

            if cleaned_text:
                return [LangChainDocument(page_content=cleaned_text, metadata={"source": url, "title": title or "ì œëª© ì—†ìŒ"})]
            return []
    except Exception as e:
        print(f"Playwrightë¡œ URL ì²˜ë¦¬ ì‹¤íŒ¨ {url}: {e}")
        return []

async def _get_documents_from_urls_async(urls: list[str]) -> list[LangChainDocument]:
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ URLì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§í•˜ê³  ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    tasks = [_scrape_url_with_playwright(url) for url in urls]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    all_documents = []
    for res in results:
        if isinstance(res, list):
            all_documents.extend(res)
        elif isinstance(res, Exception):
            print(f"URL ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {res}")
            
    return all_documents

async def load_documents(source_type: str, source_input) -> list[LangChainDocument]:
    """
    ì†ŒìŠ¤ íƒ€ì…(URL ë˜ëŠ” íŒŒì¼)ì— ë”°ë¼ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì •ì œí•©ë‹ˆë‹¤.
    """
    documents = []
    if source_type == "URL":
        urls = [url.strip() for url in source_input.splitlines() if url.strip()]
        if not urls:
            print("ì…ë ¥ëœ URLì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        print(f"ì´ {len(urls)}ê°œì˜ URL ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘ (Playwright ì‚¬ìš©)...")
        # Playwright ê¸°ë°˜ì˜ ìƒˆ í•¨ìˆ˜ í˜¸ì¶œ
        documents = await _get_documents_from_urls_async(urls)

    elif source_type == "Files":
        # íŒŒì¼ ì²˜ë¦¬ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼
        txt_files = [f for f in source_input if f.name.endswith('.txt')]
        other_files = [f for f in source_input if not f.name.endswith('.txt')]

        for txt_file in txt_files:
            try:
                content = txt_file.getvalue().decode('utf-8')
                doc = LangChainDocument(page_content=content, metadata={"source": txt_file.name, "title": txt_file.name})
                documents.append(doc)
            except Exception as e:
                print(f"Error reading .txt file {txt_file.name}: {e}")
        
        if other_files:
            print(f"{len(other_files)}ê°œì˜ íŒŒì¼(PDF, DOCX ë“±)ì„ LlamaParseë¡œ ë¶„ì„í•©ë‹ˆë‹¤...")
            llama_documents = get_documents_from_files(other_files)
            if llama_documents:
                langchain_docs = [LangChainDocument(page_content=doc.text, metadata=doc.metadata) for doc in llama_documents]
                documents.extend(langchain_docs)

                # â–¼â–¼â–¼ [ë””ë²„ê¹… ì½”ë“œ] â–¼â–¼â–¼
                print("\n\n" + "="*50)
                print("ğŸ•µï¸ 1. [data_loader] LlamaParseê°€ ì¶”ì¶œí•œ ì „ì²´ í…ìŠ¤íŠ¸ í™•ì¸")
                print(f"ì´ {len(documents)}ê°œì˜ Documentê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                for i, doc in enumerate(documents):
                    # EPS ì •ë³´ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                    if "EPS" in doc.page_content:
                        print(f"\n--- Document #{i+1} (EPS ì •ë³´ í¬í•¨ ê°€ëŠ¥ì„±) ---")
                        print(doc.page_content[:1000] + "...") # ë‚´ìš©ì´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¼ë¶€ë§Œ ì¶œë ¥
                print("="*50 + "\n\n")
                # â–²â–²â–² [ë””ë²„ê¹… ì½”ë“œ] â–²â–²â–²
    
    return documents
