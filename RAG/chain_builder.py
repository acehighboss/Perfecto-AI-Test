from langchain_core.documents import Document as LangChainDocument
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_google_genai import ChatGoogleGenerativeAI

def get_conversational_rag_chain(retriever, system_prompt):
    """
    ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ ë¬¸ì¥ ë‹¨ìœ„ì˜ ì¶œì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    """
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.1)
    
    rag_prompt_template = f"""{system_prompt}

Answer the user's request based *only* on the provided "Context".
If the context does not contain the answer, say you don't know.
Do not use any prior knowledge.

**Context:**
{{context}}

**User's Request:**
{{input}}

**Answer (in Korean):**
"""
    rag_prompt = ChatPromptTemplate.from_template(rag_prompt_template)
    
    def format_docs_with_metadata(docs: list[LangChainDocument]) -> str:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ LLM í”„ë¡¬í”„íŠ¸ í˜•ì‹ì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if not docs:
            return "No context provided."
        
        sources = {}
        for doc in docs:
            source_url = doc.metadata.get("source", "Unknown Source")
            title = doc.metadata.get("title", "No Title")
            key = (source_url, title)
            if key not in sources:
                sources[key] = []
            sources[key].append(doc.page_content)

        formatted_string = ""
        for (source_url, title), sentences in sources.items():
            formatted_string += f"\n--- Source: {title} ({source_url}) ---\n"
            formatted_string += "\n".join(f"- {s}" for s in sentences)

        # â–¼â–¼â–¼ [ë””ë²„ê¹… ì½”ë“œ] â–¼â–¼â–¼
        print("\n\n" + "="*50, flush=True)
        print("ğŸ•µï¸ 4. [chain_builder] LLMì— ì „ë‹¬ë˜ëŠ” ìµœì¢… ì»¨í…ìŠ¤íŠ¸ í™•ì¸", flush=True)
        print(formatted_string.strip(), flush=True)
        print("="*50 + "\n\n", flush=True)
        # â–²â–²â–² [ë””ë²„ê¹… ì½”ë“œ] â–²â–²â–²
        
        return formatted_string.strip()

    rag_chain = (
        {"context": retriever | RunnableLambda(format_docs_with_metadata), "input": RunnablePassthrough()}
        | rag_prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

def get_default_chain(system_prompt):
    prompt = ChatPromptTemplate.from_messages(
        [("system", system_prompt), ("user", "{question}")]
    )
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.7)
    return prompt | llm | StrOutputParser()
